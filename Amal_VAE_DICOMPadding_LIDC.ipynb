{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erg7vBWMf1t2",
        "outputId": "d8a27ceb-43dc-4ecb-918a-345eaef95225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyIUbDlDf16j",
        "outputId": "15864d3c-cf22-4241-aeca-e491f4ddd77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtCQSPo1f96Y",
        "outputId": "255af229-1c6e-4bde-8032-d1782929f42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MedIx REU/Datasets\n"
          ]
        }
      ],
      "source": [
        "cd content/drive/MyDrive/MedIx\\ REU/Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spJ7q9K10yum"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import path\n",
        "import pandas as pd\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "import sklearn.model_selection as sk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1Yx4wkIfzV5"
      },
      "outputs": [],
      "source": [
        "def getNormed(this_array, set_to_int = True):\n",
        "    this_max = this_array.max()\n",
        "    new_var = this_array.copy()\n",
        "\n",
        "    new_var = (new_var - (-2048))/(4095 - (-2048))*255\n",
        "\n",
        "    if set_to_int:\n",
        "        return new_var\n",
        "    return new_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ac8Xw6PfzV5"
      },
      "outputs": [],
      "source": [
        "def getSamePadding(this_array):\n",
        "    # Array to be added as row\n",
        "    row_to_be_added = this_array[70]\n",
        "    # Adding row to numpy array\n",
        "    result = np.vstack ((this_array, row_to_be_added) )\n",
        "\n",
        "    # Array to be added as column\n",
        "    column = this_array[:, 70]\n",
        "    last_item = this_array[70, 70]\n",
        "\n",
        "    # Adding column to numpy array\n",
        "    column_to_be_added = np.append(column, last_item)\n",
        "\n",
        "    # stack column to numpy array\n",
        "    new_img = np.column_stack((result, column_to_be_added))\n",
        "    return new_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlC9lNRZfzV6"
      },
      "source": [
        "# Reading images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV-ZTBvBfzV7",
        "outputId": "5eab640e-58a4-4730-a6f0-76a50be00cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MedIx REU/Datasets/Amal/LIDC\n",
            "Malignancy_5\n",
            "Malignancy_1\n",
            "Malignancy_4\n",
            "Malignancy_3\n",
            "Malignancy_2\n"
          ]
        }
      ],
      "source": [
        "# collect all images for training\n",
        "image_folder = '/content/drive/MyDrive/MedIx REU/Datasets/Amal/LIDC'\n",
        "image_array = []\n",
        "noduleID_array = []\n",
        "print(image_folder)\n",
        "\n",
        "for dir1 in os.listdir(image_folder):\n",
        "    print(dir1)\n",
        "    if not dir1.startswith('.'):\n",
        "        for file in os.listdir(os.path.join(image_folder, dir1)):\n",
        "\n",
        "            noduleID = file.split('.')[0]\n",
        "            noduleID_array.append(noduleID)\n",
        "            temp_image = np.loadtxt(os.path.join(image_folder, dir1,file))\n",
        "            temp_image = getNormed(temp_image)\n",
        "            if temp_image.shape[0] > 72 or temp_image.shape[1] > 72:\n",
        "                continue\n",
        "            #enlarged_img = addZeroCenter(temp_image,72-temp_image.shape[0],72-temp_image.shape[1])\n",
        "            enlarged_img = getSamePadding(temp_image)\n",
        "            image_array.append(enlarged_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftDlcWK_fzV8",
        "outputId": "7e728570-efe1-4fcb-dd22-8f65630d4fb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2680"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(noduleID_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TisFRMP9fzV8",
        "outputId": "e210134e-3272-4360-c6c5-441cef75bf90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2680"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(image_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7jgGGvsMOsV"
      },
      "outputs": [],
      "source": [
        "image_array  = np.array(image_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RS8N0OnfzV8",
        "outputId": "b4cced22-1c13-448f-cc57-04d2eda10a5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[141.3022953 , 142.46459385, 140.01546476, ..., 136.36252645,\n",
              "        134.49454664, 134.49454664],\n",
              "       [145.57789354, 145.57789354, 144.33257366, ..., 138.06446362,\n",
              "        137.10971838, 137.10971838],\n",
              "       [137.48331434, 137.524825  , 140.13999674, ..., 141.38531662,\n",
              "        142.42308318, 142.42308318],\n",
              "       ...,\n",
              "       [131.2152043 , 130.84160833, 131.04916165, ...,  96.71984372,\n",
              "        107.0559987 , 107.0559987 ],\n",
              "       [131.33973629, 130.92462966, 130.84160833, ...,  96.51229041,\n",
              "        107.0559987 , 107.0559987 ],\n",
              "       [131.33973629, 130.92462966, 130.84160833, ...,  96.51229041,\n",
              "        107.0559987 , 107.0559987 ]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_array[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1j5tR7IfzV9"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = sk.train_test_split(image_array,noduleID_array,test_size=0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgW9YCvyfzV9",
        "outputId": "1211401a-6fce-44b8-e7c5-d9b24996b149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2144\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 72, 72, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 72, 72, 1).astype('float32')\n",
        "print(len(x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD-otfuzfzV9"
      },
      "source": [
        "# Normalizing the data is important:\n",
        "- We normalize the training data by using (layers.experimental.preprocessing.Rescaling(scale= 1./255))  \n",
        "- We normalize the testing data with (x_test = x_text/255.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k505MfAAfzV-"
      },
      "source": [
        "# Shuffle 100 times:\n",
        "- The larger values like 6000 gives Nan values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqS2rLfvQeYG",
        "outputId": "62f9f404-bdb3-4131-d5b1-4e0214a3a8c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2144"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).\\\n",
        "shuffle(6000).batch(1)# batch 128 gives similar results\n",
        "\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "uI7LFcIBQekk",
        "outputId": "50677478-f31d-41b2-ba44-c60953b98bd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'plt.figure(figsize=(10, 10))\\nfor images in train_dataset.take(1):\\n    for i in range(9):\\n        ax = plt.subplot(3, 3, i + 1)\\n        plt.imshow(images[i,:,:,0], cmap=\\'gray\\')\\n        plt.axis(\"off\")'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"plt.figure(figsize=(10, 10))\n",
        "for images in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i,:,:,0], cmap='gray')\n",
        "        plt.axis(\"off\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00C05cIpQeoK",
        "outputId": "574fe716-f86e-4173-f552-1fd5cfc52e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./255.)\n",
        "normalized_ds = train_dataset.map(lambda x: normalization_layer(x))\n",
        "image_batch = next(iter(normalized_ds))\n",
        "print(len(image_batch))\n",
        "#first_image = image_batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4JOaFNiQeq5"
      },
      "outputs": [],
      "source": [
        "input_encoder = (72, 72, 1)\n",
        "input_decoder = (70,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9WmaAgnRHk8"
      },
      "outputs": [],
      "source": [
        "def sampling(input_1,input_2):\n",
        "    #input1 = layers.Lambda(sampling_model, name='encoder_output')([mean, var])\n",
        "    mean = keras.Input(shape=input_1, name='input_layer1')\n",
        "    log_var = keras.Input(shape=input_2, name='input_layer2')\n",
        "    out = layers.Lambda(sampling_model, name='encoder_output')([mean, log_var])\n",
        "    enc_2 = tf.keras.Model([mean,log_var], out,  name=\"Encoder_2\")\n",
        "    # print(enc_2.summary())\n",
        "    return enc_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMwpknmIOr4h"
      },
      "outputs": [],
      "source": [
        "def sampling_model(distribution_params):\n",
        "    mean, log_var = distribution_params\n",
        "    epsilon = K.random_normal(shape=K.shape(mean), mean=0., stddev=1.)\n",
        "    # print(epsilon)\n",
        "    z = mean + K.exp(log_var / 2) * epsilon\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMGwKYLARBAa"
      },
      "outputs": [],
      "source": [
        "def encoder(input_encoder):\n",
        "\n",
        "    inputs = keras.Input(shape=input_encoder, name='input_layer')\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides= 1, padding='same', name='conv_1')(inputs)\n",
        "    x = layers.BatchNormalization(name='bn_1')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
        "    x = layers.BatchNormalization(name='bn_2')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "    print(x.shape)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
        "    x = layers.BatchNormalization(name='bn_3')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "    print(x.shape)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(64, 3, 1, padding='same', name='conv_4')(x)\n",
        "    x = layers.BatchNormalization(name='bn_4')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    flatten = layers.Flatten()(x)\n",
        "    mean = layers.Dense(70, name='mean')(flatten)\n",
        "    log_var = layers.Dense(70, name='log_var')(flatten)\n",
        "    model = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tedJYUhcRDQ0",
        "outputId": "a0e8be68-93aa-43f3-f95b-7cb576f3e3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 36, 36, 64)\n",
            "(None, 18, 18, 64)\n",
            "(None, 18, 18, 64)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<KerasTensor: shape=(None, 70) dtype=float32 (created by layer 'mean')>,\n",
              " <KerasTensor: shape=(None, 70) dtype=float32 (created by layer 'log_var')>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc = encoder(input_encoder)\n",
        "enc.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQqqb72YRFW4"
      },
      "outputs": [],
      "source": [
        "input_1 = (70,)\n",
        "input_2 = (70,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fkhyOC8RJwi"
      },
      "outputs": [],
      "source": [
        "final = sampling(input_1,input_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ98WWxoRLdi"
      },
      "outputs": [],
      "source": [
        "def decoder(input_decoder):\n",
        "\n",
        "    inputs = keras.Input(shape=input_decoder, name='input_layer')\n",
        "    x = layers.Dense(20736, name='dense_1')(inputs)\n",
        "    x = layers.Reshape((18, 18, 64), name='Reshape_Layer')(x)\n",
        "\n",
        "    # Block-1\n",
        "    x = layers.Conv2DTranspose(64, 3, strides= 1, padding='same',name='conv_transpose_1')(x)\n",
        "    x = layers.BatchNormalization(name='bn_1')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "    # Block-2\n",
        "    x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
        "    x = layers.BatchNormalization(name='bn_2')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    # Block-3\n",
        "    x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
        "    x = layers.BatchNormalization(name='bn_3')(x)\n",
        "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "    # Block-4\n",
        "    outputs = layers.Conv2DTranspose(1, 3, 1,padding='same', activation='sigmoid', name='conv_transpose_4')(x)\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfDswRnIRNWR",
        "outputId": "0f811de7-4ced-4fc0-936f-8533fae155e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 36, 36, 64)\n"
          ]
        }
      ],
      "source": [
        "dec = decoder(input_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DU1gunnfzWA",
        "outputId": "018483bd-e049-4280-c327-6df88d7c4572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 70)]              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20736)             1472256   \n",
            "                                                                 \n",
            " Reshape_Layer (Reshape)     (None, 18, 18, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_1 (Conv2DTra  (None, 18, 18, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_1 (BatchNormalization)   (None, 18, 18, 64)        256       \n",
            "                                                                 \n",
            " lrelu_1 (LeakyReLU)         (None, 18, 18, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_2 (Conv2DTra  (None, 36, 36, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_2 (BatchNormalization)   (None, 36, 36, 64)        256       \n",
            "                                                                 \n",
            " lrelu_2 (LeakyReLU)         (None, 36, 36, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_3 (Conv2DTra  (None, 72, 72, 32)       18464     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_3 (BatchNormalization)   (None, 72, 72, 32)        128       \n",
            "                                                                 \n",
            " lrelu_3 (LeakyReLU)         (None, 72, 72, 32)        0         \n",
            "                                                                 \n",
            " conv_transpose_4 (Conv2DTra  (None, 72, 72, 1)        289       \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,565,505\n",
            "Trainable params: 1,565,185\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "dec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df1lfM7VRPCm",
        "outputId": "94acf73a-0332-49a9-8393-04620e254fc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQngjmReRQ1u"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "    #r_loss = K.sum(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "    #print('y_true Shape',y_true.shape)\n",
        "    # print('r_loss', K.eval(r_loss*1000))\n",
        "    return 1000*r_loss\n",
        "\n",
        "def kl_loss_fun(mean, log_var):\n",
        "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis = 1)\n",
        "    return kl_loss\n",
        "\n",
        "def vae_loss(y_true, y_pred, mean, log_var):\n",
        "    r_loss = mse_loss(y_true, y_pred)\n",
        "    kl_loss = kl_loss_fun(mean, log_var)\n",
        "    # print('kl_loss', K.eval(kl_loss))\n",
        "    #print('total_loss',K.eval(r_loss + kl_loss))\n",
        "    return  r_loss + kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgKk1KQtRSsW"
      },
      "outputs": [],
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "\n",
        "    with tf.GradientTape() as encoder, tf.GradientTape() as decoder:\n",
        "\n",
        "        mean, log_var = enc(images, training=True)\n",
        "\n",
        "        #print(enc.weights[0][0][0])\n",
        "        latent = final([mean, log_var])\n",
        "        generated_images = dec(latent, training=True)\n",
        "        loss = vae_loss(images, generated_images, mean, log_var)\n",
        "\n",
        "\n",
        "    gradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
        "    gradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
        "\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
        "    optimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsriijgBRUnO"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/MedIx REU/Datasets/Amal/VAE/Results/training_weights', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/MedIx REU/Datasets/Amal/VAE/Results/images', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4AMOgVI7sHR"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "    m, v = enc(test_input, training=False)\n",
        "    latent = final([m,  v])\n",
        "    predictions = dec(latent, training=False)\n",
        "    #print(predictions.shape)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0]* 255, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # plt.savefig('/content/drive/MyDrive/Colab Notebooks/VAE/images/image_at_epoch_{:d}.png'.format(epoch))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyVhdBWqRXA-"
      },
      "outputs": [],
      "source": [
        "loss_ = []\n",
        "epoch_loss = []\n",
        "loss_mean = []\n",
        "def train(dataset, epochs):\n",
        "    #print(len(dataset))\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        i = 0\n",
        "        for image_batch in dataset:\n",
        "            i += 1\n",
        "            loss = train_step(image_batch)\n",
        "            loss_.append(loss)\n",
        "\n",
        "            #seed = image_batch[:25]\n",
        "            #display.clear_output(wait=True)\n",
        "            #generate_and_save_images([enc,final,dec],epoch + 1,seed)\n",
        "\n",
        "        # Save the model every 15 epochs\n",
        "        #if (epoch + 1) % 15 == 0:\n",
        "        #checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        # Create count of the number of epochs\n",
        "\n",
        "        enc.save_weights('/content/drive/MyDrive/MedIx REU/Datasets/Amal/VAE/Results/training_weights/enc_'+ str(epoch)+'.h5')\n",
        "        dec.save_weights('/content/drive/MyDrive/MedIx REU/Datasets/Amal/VAE/Results/training_weights/dec_'+ str(epoch)+'.h5')\n",
        "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "    #enc.save('/Users/amalalmansour/Downloads/VAE/enc_VAE_my_model')\n",
        "    #dec.save('/Users/amalalmansour/Downloads/VAE/dec_VAE_my_model')\n",
        "\n",
        "        # Generate after the final epoch\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images([enc,final,dec],\n",
        "                            epochs,\n",
        "                            seed)\n",
        "    # Create count of the number of epochs\n",
        "    '''for epoch in range(epochs):\n",
        "        acc_loss = 0.\n",
        "        for batch in range(128):\n",
        "            # do the training\n",
        "            acc_loss += flatten_list[batch]\n",
        "        epoch_loss.append(acc_loss / 128)'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcUyDwVdfzWC"
      },
      "source": [
        "# Reduce the number of epochs:\n",
        "- 2 epoches are the best threshold to better visualize the reconstructed images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdhSdjZaRZCW",
        "outputId": "a987f861-d985-4521-fd6e-ae81b3762418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 109.2079005241394 sec\n",
            "Time for epoch 2 is 144.92710709571838 sec\n",
            "Time for epoch 3 is 144.87173914909363 sec\n",
            "Time for epoch 4 is 93.9288182258606 sec\n",
            "Time for epoch 5 is 144.9345428943634 sec\n",
            "Time for epoch 6 is 92.22281789779663 sec\n",
            "Time for epoch 7 is 145.22067713737488 sec\n",
            "Time for epoch 8 is 93.0492525100708 sec\n",
            "Time for epoch 9 is 92.53000020980835 sec\n",
            "Time for epoch 10 is 144.5392394065857 sec\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6cdafd74d7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_functions_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-72f2fe061ed9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     generate_and_save_images([enc,final,dec],\n\u001b[1;32m     33\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                             seed)\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Create count of the number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     '''for epoch in range(epochs):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
          ]
        }
      ],
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "train(normalized_ds, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rch6OzODfzWC"
      },
      "outputs": [],
      "source": [
        "# Visualize loss history\n",
        "img_num = 2144\n",
        "epoch_loss = []\n",
        "acc_loss = 0.\n",
        "for epoch in range(100):\n",
        "    for batch in range(len(loss_)):\n",
        "        acc_loss += loss_[batch].numpy()\n",
        "    epoch_loss.append(float(acc_loss/2144))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mk0smSCUfzWD"
      },
      "outputs": [],
      "source": [
        "(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2H5KQWSWfzWE"
      },
      "outputs": [],
      "source": [
        "epoch_count = range(1, len(epoch_loss) + 1)\n",
        "plt.plot(epoch_count,epoch_loss)\n",
        "plt.legend(['Training Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j3L2vXLrfzWE"
      },
      "outputs": [],
      "source": [
        "epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8sObI6zrY0Ii"
      },
      "outputs": [],
      "source": [
        "list_data = list(normalized_ds)\n",
        "list_data_copy = list_data\n",
        "\n",
        "print(len(list_data_copy))\n",
        "\n",
        "mean, log_var = enc(list_data_copy[12], training=False)\n",
        "latent = final([mean, log_var])\n",
        "generated_images = dec(latent, training=False)\n",
        "plt.imshow(np.array(list_data_copy[12][0, :,:,0]* 255))\n",
        "plt.gray()\n",
        "plt.axis(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2cBGJplofzWF"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.array(generated_images[0, :,:,0]*255))\n",
        "plt.gray()\n",
        "plt.axis(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ne4-a2ZoY0C2"
      },
      "outputs": [],
      "source": [
        "#np.unique(getNormed(np.array(generated_images)))\n",
        "np.array(generated_images[0, :,:,0]* 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Db4NfAUm3Lx"
      },
      "outputs": [],
      "source": [
        "enc = encoder(input_encoder)\n",
        "enc.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G0bdqm5OSXvf"
      },
      "outputs": [],
      "source": [
        "enc.load_weights('/Users/amalalmansour/Downloads/VAE/Results/training_weights/enc_99.h5')\n",
        "dec.load_weights('/Users/amalalmansour/Downloads/VAE/Results/training_weights/dec_99.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YhxvWVfzWG"
      },
      "source": [
        "# Original training images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TBe7-wFIfzWG"
      },
      "outputs": [],
      "source": [
        "m, v = enc.predict(x_train[:10]/255)\n",
        "latent = final([m,v])\n",
        "reconst = dec.predict(latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vAOm5rFefzWH"
      },
      "outputs": [],
      "source": [
        "n = 10  ## how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ## display original\n",
        "    ax = plt.subplot(2, n, i+ 1)\n",
        "    ax.set_title(\"Original Image\")\n",
        "    plt.imshow(x_train[i, :,:,0]* 255)\n",
        "    plt.gray()\n",
        "    plt.axis(False)\n",
        "\n",
        "    ## display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    ax.set_title(\"Predicted Image\")\n",
        "    plt.imshow(np.array(reconst[i, :,:,0]* 255))\n",
        "    plt.gray()\n",
        "    plt.axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssN7NNmfzWH"
      },
      "source": [
        "# Testing data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ8ysF1-fzWH"
      },
      "source": [
        "### Original testing images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OywfCaepfzWH"
      },
      "outputs": [],
      "source": [
        "figsize = 15\n",
        "m,v = enc.predict(x_test[:10]/255)\n",
        "latent = final([m,v])\n",
        "reconst_test = dec.predict(latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cGzEo8eCfzWI"
      },
      "outputs": [],
      "source": [
        "n = 10  ## how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ## display original\n",
        "    ax = plt.subplot(2, n, i+ 1)\n",
        "    ax.set_title(\"Original Image\")\n",
        "    plt.imshow(x_test[i, :,:,0])\n",
        "    plt.gray()\n",
        "    plt.axis(False)\n",
        "\n",
        "    ## display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    ax.set_title(\"Predicted Image\")\n",
        "    plt.imshow(np.array(reconst_test[i, :,:,0]* 255))\n",
        "    plt.gray()\n",
        "    plt.axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEDZWqS5fzWI"
      },
      "source": [
        "## Reconstructing  Images with Latent-Vector Sampled from Normal Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3KCgSdQ3fzWI"
      },
      "outputs": [],
      "source": [
        "figsize = 15\n",
        "\n",
        "x = np.random.normal(size = (25,70))\n",
        "reconstruct = dec.predict(x)\n",
        "fig = plt.figure(figsize=(figsize, 10))\n",
        "\n",
        "for i in range(25):\n",
        "    ax = fig.add_subplot(5, 5, i+1)\n",
        "    ax.axis('off')\n",
        "    ax.imshow(np.array(reconstruct[i, :,:,0]* 255), cmap = 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qzs3Lf0HfzWI"
      },
      "outputs": [],
      "source": [
        "n_to_show = 5000\n",
        "figsize = 12\n",
        "example_idx = np.random.choice(range(len(x_test)), n_to_show)\n",
        "example_images = x_test[example_idx]\n",
        "\n",
        "m, v = enc.predict(example_images)\n",
        "embeddings = final([m,v])\n",
        "\n",
        "plt.figure(figsize=(figsize, figsize))\n",
        "plt.scatter(embeddings[:, 0] , embeddings[:, 1], alpha=0.5, s=2)\n",
        "plt.xlabel(\"Dimension-1\", size=20)\n",
        "plt.ylabel(\"Dimension-2\", size=20)\n",
        "plt.xticks(size=20)\n",
        "plt.yticks(size=20)\n",
        "plt.title(\"Projection of 2D Latent-Space (LIDC)\", size=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "69JL2WUPfzWI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}